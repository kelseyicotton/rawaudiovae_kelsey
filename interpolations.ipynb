{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolations Script for Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pathlib import Path\n",
    "import configparser\n",
    "\n",
    "from rawvae.model import VAE\n",
    "print(\"Imports done! ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(config_path):\n",
    "    \"\"\"\n",
    "    Read and parse arguments from configuration file\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nReading configuration file from: {config_path}\")\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_path)\n",
    "    print(\"Params loaded\")\n",
    "\n",
    "    # Extract model parameters from config\n",
    "    model_params = {\n",
    "        'segment_length': config.getint('audio', 'segment_length'),\n",
    "        'sampling_rate': config.getint('audio', 'sampling_rate'),\n",
    "        'n_units': config.getint('VAE', 'n_units'),\n",
    "        'latent_dim': config.getint('VAE', 'latent_dim'),\n",
    "        'batch_size': config.getint('training', 'batch_size'),\n",
    "        'learning_rate': config.getfloat('training', 'learning_rate'),\n",
    "        'epochs': config.getint('training', 'epochs')\n",
    "    }\n",
    "\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_audio(audio_dir):\n",
    "    \"\"\"\n",
    "    Random pick from directory\n",
    "    \"\"\"\n",
    "\n",
    "    audio_files = list(Path(audio_dir).glob('*.wav'))\n",
    "\n",
    "    if len(audio_files) < 2:\n",
    "        raise ValueError(f\"Not enough files in {audio_dir} to facilitate random choice\")\n",
    "    \n",
    "    audio1 = np.random.choice(audio_files)\n",
    "\n",
    "    remaining_files = [f for f in audio_files if f != audio1]\n",
    "    audio2 = np.random.choice(remaining_files)\n",
    "\n",
    "    print(f\"File 1: {audio1.name}\\nFile2: {audio2.name}\")\n",
    "\n",
    "    return str(audio1), str(audio2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_audio_lengths(audio1, audio2):\n",
    "    \"\"\"\n",
    "    Match audio lengths\n",
    "    Just concatenate shorter audio until length of longer file is met\n",
    "    \"\"\"\n",
    "\n",
    "    len_audio1, len_audio2 = len(audio1), len(audio2)\n",
    "\n",
    "    if len_audio1 == len_audio2:\n",
    "        return audio1, audio2\n",
    "\n",
    "    if len_audio1 < len_audio2:\n",
    "\n",
    "        # Calculate how many full repeats needed\n",
    "        repeats = len_audio2 // len_audio1\n",
    "        remainder = len_audio2 % len_audio1\n",
    "\n",
    "        # Concatenate\n",
    "        matched_audio = np.tile(audio1, repeats)\n",
    "\n",
    "        if remainder > 0:\n",
    "            matched_audio = np.concatenate([matched_audio, audio1[:remainder]])\n",
    "        return matched_audio, audio2\n",
    "    else:\n",
    "\n",
    "        repeats = len_audio1 // len_audio2\n",
    "        remainder = len_audio1 % len_audio2\n",
    "\n",
    "        matched_audio = np.tile(audio2, repeats)\n",
    "        if remainder > 0:\n",
    "            matched_audio = np.concatenate([matched_audio, audio2[:remainder]])\n",
    "        return matched_audio, audio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_full_audio(audio, segment_length):\n",
    "    \"\"\"\n",
    "    Process full audios into segments and return tensor\n",
    "    \"\"\"\n",
    "\n",
    "    n_segments = len(audio) // segment_length\n",
    "    audio = audio[:n_segments * segment_length]\n",
    "    segments = audio.reshape(n_segments, segment_length)\n",
    "\n",
    "    return torch.FloatTensor(segments)\n",
    "\n",
    "def load_process_audio(file_path, segment_length, sr=44100):\n",
    "    \"\"\"\n",
    "    Load and process audio file (mainly normalization and shit)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nProcessing audio file: {file_path}\")\n",
    "    print(f\"Sample rate: {sr}\")\n",
    "\n",
    "    audio, _ = librosa.load(file_path, sr=sr)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET LATENT VECTOR\n",
    "def get_latent_vector(model, audio_tensor, device):\n",
    "    \"\"\"\n",
    "    Encode audio into latent vector\n",
    "    \"\"\"\n",
    "    audio_tensor = audio_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        mu, _ = model.encode(audio_tensor)\n",
    "        print(f\"Latent vector shape: {mu.shape}\")\n",
    "        print(f\"Latent vector mean: {mu.mean().item():.3f}\")\n",
    "        print(f\"Latent vector std: {mu.std().item():.3f}\")\n",
    "        return mu # using mean distibution as latent vector!\n",
    "        \n",
    "# INTERPOLATE VECTORS\n",
    "\n",
    "def interpolate_vectors(z1, z2, alpha):\n",
    "    \"\"\" \n",
    "    Interpolate between 2 latent factors \n",
    "    aplha is our interpolation factor (0 = z1, 1 = z2)\n",
    "    \"\"\"\n",
    "    interp = (1 - alpha) * z1 + alpha * z2\n",
    "    print(f\"Interpolation at alpha={alpha:.1f}\")\n",
    "    print(f\"z1 mean: {z1.mean().item():.3f}, z2 mean: {z2.mean().item():.3f}\")\n",
    "    print(f\"Interpolated mean: {interp.mean().item():.3f}\")\n",
    "    return interp\n",
    "\n",
    "def create_interpolations(model, z1, z2, sampling_rate, output_dir):\n",
    "    \"\"\"\n",
    "    Create interpolations between 2 latent vectors.\n",
    "    Save audio, save spectrogram and waveform \n",
    "    \"\"\"\n",
    "\n",
    "    # \n",
    "    # Create output dirs\n",
    "    output_dir = Path(output_dir)\n",
    "    os.makedirs(output_dir / \"audio\", exist_ok=True)\n",
    "    os.makedirs(output_dir / \"spectrograms\", exist_ok=True)\n",
    "    os.makedirs(output_dir / \"waveforms\", exist_ok=True)\n",
    "\n",
    "    # Interpolation stations\n",
    "    alphas = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "    for alpha in alphas:\n",
    "        # Interpolate baby\n",
    "        print(f\"\\nProcessing interpolation α={alpha:.1f}\")\n",
    "        z_interp = interpolate_vectors(z1, z2, alpha)\n",
    "\n",
    "        # Decode\n",
    "        with torch.no_grad():\n",
    "            audio_interp = model.decode(z_interp)\n",
    "            audio_np = audio_interp.cpu().numpy().flatten()\n",
    "            print(f\"Decoded audio range: [{audio_np.min():.3f}, {audio_np.max():.3f}]\")\n",
    "            print(f\"Decoded audio mean: {audio_np.mean():.3f}\")\n",
    "            print(f\"Decoded audio std: {audio_np.std():.3f}\")\n",
    "\n",
    "        # Save audio\n",
    "        audio_path = output_dir / \"audio\" / f\"interpolation_{alpha:.1f}.wav\"\n",
    "        sf.write(audio_path, audio_np, sampling_rate)\n",
    "        print(f\"Saved audio in: {audio_path}\")\n",
    "\n",
    "        # Create and save spectrogram\n",
    "        spec_path = output_dir / \"spectrograms\" / f\"spectrogram_{alpha:.1f}.png\"\n",
    "        plt.figure(figsize=(10,4))\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_np)), ref=np.max)\n",
    "        librosa.display.specshow(D, y_axis='log', x_axis='time')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(f'Spectrogram ((α={alpha:.1f})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / \"spectrograms\" / f\"spectrogram_{alpha:.1f}.png\")\n",
    "        plt.close()\n",
    "        print(f\"Saved spectrogram: {spec_path}\")\n",
    "\n",
    "        # Create and save waveform\n",
    "        wave_path = output_dir / \"waveforms\" / f\"waveform_{alpha:.1f}.png\"\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(audio_np)\n",
    "        plt.title(f'Waveform (α={alpha:.1f})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / \"waveforms\" / f\"waveform_{alpha:.1f}.png\")\n",
    "        plt.close()\n",
    "        print(f\"Saved waveform: {wave_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"\\n ↗️=== Starting Audio Interpolation Process ===↙️\")\n",
    "\n",
    "    # Load trained model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create output directories at the start\n",
    "    output_dir = Path(\"./interpolations\")\n",
    "    os.makedirs(output_dir / \"audio\", exist_ok=True)\n",
    "    os.makedirs(output_dir / \"spectrograms\", exist_ok=True)\n",
    "    os.makedirs(output_dir / \"waveforms\", exist_ok=True)\n",
    "    print(\"Created output directories\")\n",
    "\n",
    "    # Read configuration file for our model\n",
    "    config_path = r\"D:\\kelse\\03_Repositories\\RAWAUDIOVAE_PROJECT\\KELSEY_DEV\\rawaudiovae_kelsey\\rawaudiovae_BASE\\content\\kelsey\\config.ini\"\n",
    "    # REPLACE THIS EVENTUALLY WITH READING BACK FROM CONFIG FILE\n",
    "\n",
    "    model_params = read_config(config_path)\n",
    "\n",
    "    # EXTRACT AND ASSIGN PARAMS\n",
    "    segment_length = model_params['segment_length']\n",
    "    n_units = model_params['n_units']\n",
    "    latent_dim = model_params['latent_dim']\n",
    "    sampling_rate = model_params['sampling_rate']\n",
    "    print(\"Loaded model parameters from config:\")\n",
    "    print(f\"Segment Length: {segment_length}\")\n",
    "    print(f\"Number of Units: {n_units}\")\n",
    "    print(f\"Latent Dimension: {latent_dim}\")\n",
    "    print(f\"Sampling Rate: {sampling_rate}\")\n",
    "\n",
    "    # LOAD MODEL, CHECK PATH CORRECT\n",
    "    print(f\"\\nLoading model checkpoint... ⌛\")\n",
    "    checkpoint_path = r'D:\\kelse\\03_Repositories\\RAWAUDIOVAE_PROJECT\\KELSEY_DEV\\rawaudiovae_kelsey\\rawaudiovae_BASE\\content\\kelsey\\checkpoints\\ckpt_00150'\n",
    "    # REPLACE THIS EVENTUALLY WITH READING BACK FROM LAST MODEL RUN LOG\n",
    "\n",
    "    print(f\"Checkpoint located at: {checkpoint_path}\")\n",
    "    \n",
    "    state = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # INITIALISE\n",
    "    print(\"Initialising model 🚀\")\n",
    "    model = VAE(segment_length, n_units, latent_dim).to(device)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    model.eval()\n",
    "    print(\"Locked and loaded ⛑️\")\n",
    "\n",
    "    # AUDIO DIR\n",
    "    audio_dir = r\"D:\\kelse\\03_Repositories\\RAWAUDIOVAE_PROJECT\\KELSEY_DEV\\rawaudiovae_kelsey\\rawaudiovae_BASE\\audio_experiments\"\n",
    "    # REPLACE THIS EVENTUALLY WITH READING BACK TEST_AUDIO FROM CONFIG FILE\n",
    "    audio_file1, audio_file2 = get_random_audio(audio_dir)\n",
    "    \n",
    "    # PROCESS\n",
    "    audio1 = load_process_audio(audio_file1, segment_length, sampling_rate)\n",
    "    audio2 = load_process_audio(audio_file2, segment_length, sampling_rate)\n",
    "\n",
    "    # MATCH\n",
    "    audio1_matched, audio2_matched = match_audio_lengths(audio1, audio2)\n",
    "    print(f\"Matched lengths: {len(audio1_matched)} samples\")\n",
    "\n",
    "    # TENSOR BUSINESS\n",
    "    print(\"\\nProcessing input audio files...!\")\n",
    "    audio_tensor1 = process_full_audio(audio1_matched, segment_length)\n",
    "    audio_tensor2 = process_full_audio(audio2_matched, segment_length)\n",
    "    print(f\"Created tensors of shape: {audio_tensor1.shape}\")\n",
    "\n",
    "    # GET LATENT VECTORS\n",
    "    print(\"\\nGenerating latent vectors 👩‍🍳\")\n",
    "    z1 = get_latent_vector(model, audio_tensor1, device)\n",
    "    z2 = get_latent_vector(model, audio_tensor2, device)\n",
    "\n",
    "    # Added this section to test encode-decode of original files\n",
    "    print(\"\\nTesting encode-decode of original files...\")\n",
    "    with torch.no_grad():\n",
    "        # First file\n",
    "        print(\"\\nProcessing file 1: \")\n",
    "        decoded1 = model.decode(z1)\n",
    "        print(f\"Decoded tensor shape: {decoded1.shape}\")\n",
    "        audio_np1 = decoded1.cpu().numpy().flatten()\n",
    "\n",
    "        # AUDIO\n",
    "        sf.write(\"./interpolations/audio/original1_encoded_decoded.wav\", audio_np1, sampling_rate)\n",
    "        print(\"Saved encoded-decoded version of first file\")\n",
    "        \n",
    "        # SPECTROGRAM\n",
    "        plt.figure(figsize=(10,4))\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_np1)), ref=np.max)\n",
    "        librosa.display.specshow(D, y_axis='log', x_axis='time')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Original 1 Encoded-Decoded Spectrogram')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./interpolations/spectrograms/original1_encoded_decoded.png\")\n",
    "        plt.close()\n",
    "        print(\"Saved spectrogram of encoded-decoded first file\")\n",
    "        \n",
    "        # WAVEFORM\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(audio_np1)\n",
    "        plt.title('Original 1 Encoded-Decoded Waveform')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./interpolations/waveforms/original1_encoded_decoded.png\")\n",
    "        plt.close()\n",
    "        print(\"Saved waveform of encoded-decoded first file\")\n",
    "\n",
    "        # Second file\n",
    "        print(\"\\nProcessing file 2: \")\n",
    "        decoded2 = model.decode(z2)\n",
    "        print(f\"Decoded tensor shape: {decoded2.shape}\")\n",
    "        audio_np2 = decoded2.cpu().numpy().flatten()\n",
    "\n",
    "        # AUDIO\n",
    "        sf.write(\"./interpolations/audio/original2_encoded_decoded.wav\", audio_np2, sampling_rate)\n",
    "        print(\"Saved encoded-decoded version of second file\")\n",
    "\n",
    "        # SPECTROGRAM\n",
    "        plt.figure(figsize=(10,4))\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_np2)), ref=np.max)\n",
    "        librosa.display.specshow(D, y_axis='log', x_axis='time')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Original 2 Encoded-Decoded Spectrogram')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./interpolations/spectrograms/original2_encoded_decoded.png\")\n",
    "        plt.close()\n",
    "        print(\"Saved spectrogram of encoded-decoded second file\")\n",
    "        \n",
    "        # WAVEFORM\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(audio_np2)\n",
    "        plt.title('Original 2 Encoded-Decoded Waveform')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./interpolations/waveforms/original2_encoded_decoded.png\")\n",
    "        plt.close()\n",
    "        print(\"Saved waveform of encoded-decoded second file\")\n",
    "\n",
    "\n",
    "    # INTERPOLATE BABY\n",
    "    create_interpolations(model, z1, z2, sampling_rate, \"./interpolations\")\n",
    "    print(f\"Interpolation complete! Check folder for results.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kelco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
