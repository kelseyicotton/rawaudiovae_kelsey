{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1gkDKBGeaGH3"
   },
   "source": [
    "# rawaudiovae"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset and pretrained models can be found here: \n",
    "\n",
    "https://drive.google.com/file/d/1e_X2Ir26iypSdSa6pRCJBy2q5t9zXFBb/view?usp=drive_link\n",
    "\n",
    "Please download this folder and unzip it under the ./content folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-4JbrvN_UVe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchaudio\n",
    "Resample = torchaudio.transforms.Resample(44100, 48000, resampling_method='kaiser_window')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    my_cuda = 1\n",
    "else: \n",
    "    device = 'cpu'\n",
    "    my_cuda = 0\n",
    "    \n",
    "Resample = Resample.to(device)\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import interpolate as sp_interpolate\n",
    "import json\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import soundfile as sf\n",
    "# import sounddevice as sd\n",
    "import configparser\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zR4gkZoSsrbG"
   },
   "outputs": [],
   "source": [
    "sampling_rate = 44100\n",
    "sr = sampling_rate\n",
    "\n",
    "hop_length = 128\n",
    "\n",
    "segment_length = 1024\n",
    "n_units = 2048\n",
    "latent_dim = 256\n",
    "device = 'cuda:0'\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "audio_fold = Path(r'./content/2022-zkm-workshop/ltsp/erokia/audio')\n",
    "audio = audio_fold\n",
    "lts_audio_files = [f for f in audio_fold.glob('*.wav')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSInHDvAu2_-",
    "outputId": "ebab8eac-e201-4b43-874c-ca92bc0ba06b"
   },
   "outputs": [],
   "source": [
    "# Following should give you more than 0. Otherwise, the dataset is not in the right place. Please make sure that the following folder is there: rawaudiovae/content/2022-zkm-workshop\n",
    "\n",
    "len(lts_audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdUWpV22trrp"
   },
   "outputs": [],
   "source": [
    "# Models \n",
    "\n",
    "class raw_VAE(nn.Module):\n",
    "  def __init__(self, segment_length, n_units, latent_dim):\n",
    "    super(raw_VAE, self).__init__()\n",
    "\n",
    "    self.segment_length = segment_length\n",
    "    self.n_units = n_units\n",
    "    self.latent_dim = latent_dim\n",
    "    \n",
    "    self.fc1 = nn.Linear(segment_length, n_units)\n",
    "    self.fc21 = nn.Linear(n_units, latent_dim)\n",
    "    self.fc22 = nn.Linear(n_units, latent_dim)\n",
    "    self.fc3 = nn.Linear(latent_dim, n_units)\n",
    "    self.fc4 = nn.Linear(n_units, segment_length)\n",
    "\n",
    "  def encode(self, x):\n",
    "      h1 = F.relu(self.fc1(x))\n",
    "      return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "  def reparameterize(self, mu, logvar):\n",
    "      std = torch.exp(0.5*logvar)\n",
    "      eps = torch.randn_like(std)\n",
    "      return mu + eps*std\n",
    "\n",
    "  def decode(self, z):\n",
    "      h3 = F.relu(self.fc3(z))\n",
    "      return F.tanh(self.fc4(h3))\n",
    "\n",
    "  def forward(self, x):\n",
    "      mu, logvar = self.encode(x.view(-1, self.segment_length))\n",
    "      z = self.reparameterize(mu, logvar)\n",
    "      return self.decode(z), mu, logvar\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar, kl_beta, segment_length):\n",
    "  recon_loss = F.mse_loss(recon_x, x.view(-1, segment_length))\n",
    "\n",
    "  # see Appendix B from VAE paper:\n",
    "  # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "  # https://arxiv.org/abs/1312.6114\n",
    "  # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "  KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "  return recon_loss + ( kl_beta * KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6auOc8Zqtx-r"
   },
   "outputs": [],
   "source": [
    "# Datasets \n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is the main class that calculates the spectrogram and returns the\n",
    "    spectrogram, audio pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, audio_np, segment_length, sampling_rate, hop_size, transform=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.segment_length = segment_length\n",
    "        self.hop_size = hop_size\n",
    "        \n",
    "        if segment_length % hop_size != 0:\n",
    "            raise ValueError(\"segment_length {} is not a multiple of hop_size {}\".format(segment_length, hop_size))\n",
    "\n",
    "        if len(audio_np) % hop_size != 0:\n",
    "            num_zeros = hop_size - (len(audio_np) % hop_size)\n",
    "            audio_np = np.pad(audio_np, (0, num_zeros), 'constant', constant_values=(0,0))\n",
    "\n",
    "        self.audio_np = audio_np\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Take segment\n",
    "        seg_start = index * self.hop_size\n",
    "        seg_end = (index * self.hop_size) + self.segment_length\n",
    "        sample = self.audio_np[ seg_start : seg_end ]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.audio_np) // self.hop_size) - (self.segment_length // self.hop_size) + 1\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return torch.from_numpy(sample)\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is the main class that calculates the spectrogram and returns the\n",
    "    spectrogram, audio pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, audio_np, segment_length, sampling_rate, transform=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.segment_length = segment_length\n",
    "        \n",
    "        if len(audio_np) % segment_length != 0:\n",
    "            num_zeros = segment_length - (len(audio_np) % segment_length)\n",
    "            audio_np = np.pad(audio_np, (0, num_zeros), 'constant', constant_values=(0,0))\n",
    "\n",
    "        self.audio_np = audio_np\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Take segment\n",
    "        seg_start = index * self.segment_length\n",
    "        seg_end = (index * self.segment_length) + self.segment_length\n",
    "        sample = self.audio_np[ seg_start : seg_end ]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_np) // self.segment_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAK-ZayJs37R",
    "outputId": "78b63eea-6f03-4ea1-dff1-8217a12ad6a8"
   },
   "outputs": [],
   "source": [
    "state = torch.load(Path(r'./content/2022-zkm-workshop/nospectral/erokia/spectralvae/run-000/checkpoints/ckpt_00500'), map_location=torch.device(device))\n",
    "if my_cuda:\n",
    "    raw_model = raw_VAE(segment_length, n_units, latent_dim).to(device)\n",
    "else:\n",
    "    raw_model = raw_VAE(segment_length, n_units, latent_dim)\n",
    "raw_model.load_state_dict(state['state_dict'])\n",
    "raw_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_0X8tm1s6Yp"
   },
   "source": [
    "## Stepwise Interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKib2gtKtRd6"
   },
   "outputs": [],
   "source": [
    "# Load audio files ...\n",
    "\n",
    "test_audio_1_path = lts_audio_files[random.randint(0, len(lts_audio_files) - 1)]\n",
    "test_audio_1, fs = librosa.load(test_audio_1_path, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "id": "EWc6rIvltRd8",
    "outputId": "834b5e20-c0e7-43be-a261-1c58df7b1078"
   },
   "outputs": [],
   "source": [
    "display.Audio(test_audio_1, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JRi7WzdtRd8"
   },
   "outputs": [],
   "source": [
    "test_audio_2_path = lts_audio_files[random.randint(0, len(lts_audio_files)- 1)]\n",
    "test_audio_2, fs = librosa.load(test_audio_2_path, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "id": "LbZCfyFjtRd9",
    "outputId": "f202fffc-30ba-4304-b12a-93ce0f3d93b6"
   },
   "outputs": [],
   "source": [
    "display.Audio(test_audio_2, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZ7lFDlBtheZ"
   },
   "outputs": [],
   "source": [
    "# We should match the audio lengths\n",
    "# 0 is crop the longer, \n",
    "# 1 is repeat the shorter, \n",
    "#  \n",
    "\n",
    "# TODO make this an array/tensor of two - check if padding functions have these \n",
    "\n",
    "match_size = 1\n",
    "\n",
    "if match_size == 0:\n",
    "    if test_audio_1.shape[0] < test_audio_2.shape[0]:\n",
    "        test_audio_2 = test_audio_2[:test_audio_1.shape[0]]\n",
    "    else:\n",
    "        test_audio_1 = test_audio_1[:test_audio_2.shape[0]]\n",
    "\n",
    "if match_size == 1:\n",
    "    if test_audio_1.shape[0] < test_audio_2.shape[0]:\n",
    "        while test_audio_1.shape[0] < test_audio_2.shape[0]:\n",
    "            test_audio_1 = np.concatenate((test_audio_1, test_audio_1), 0)\n",
    "        test_audio_1 = test_audio_1[:test_audio_2.shape[0]] \n",
    "\n",
    "    else:\n",
    "        while test_audio_2.shape[0] < test_audio_1.shape[0]:\n",
    "            test_audio_2 = np.concatenate((test_audio_2, test_audio_2), 0)\n",
    "        test_audio_2 = test_audio_2[:test_audio_1.shape[0]]\n",
    "\n",
    "# Create the dataset\n",
    "test_dataset1 = TestDataset(test_audio_1, segment_length = segment_length, sampling_rate = sampling_rate, transform=ToTensor())\n",
    "test_dataset2 = TestDataset(test_audio_2, segment_length = segment_length, sampling_rate = sampling_rate, transform=ToTensor())\n",
    "\n",
    "test_dataloader1 = DataLoader(test_dataset1, batch_size = batch_size, shuffle=False)\n",
    "test_dataloader2 = DataLoader(test_dataset2, batch_size = batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jbNJP14tlfp"
   },
   "outputs": [],
   "source": [
    "def raw_to_z_dist(test_dataloader, raw_model, device):\n",
    "    init_test = True\n",
    "    for iterno, test_sample in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            test_sample = test_sample.to(device)\n",
    "            test_mu, test_logvar = raw_model.encode(test_sample)\n",
    "\n",
    "        if init_test:\n",
    "            test_z_mu = test_mu \n",
    "            test_z_logvar = test_logvar\n",
    "            init_test = False\n",
    "\n",
    "        else:\n",
    "            test_z_mu = torch.cat((test_z_mu, test_mu ),0)\n",
    "            test_z_logvar = torch.cat((test_z_logvar, test_logvar ),0)\n",
    "    return test_z_mu, test_z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HU3q9kLMRJkW"
   },
   "outputs": [],
   "source": [
    "# Z dist\n",
    "\n",
    "test1_z_mu, test1_z_logvar = raw_to_z_dist(test_dataloader1, raw_model, device)\n",
    "test2_z_mu, test2_z_logvar = raw_to_z_dist(test_dataloader2, raw_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRwJH7BPvpGc"
   },
   "outputs": [],
   "source": [
    "def raw_interpolate_stepwise_z_dist(test1_z_mu, test1_z_logvar, test2_z_mu, test2_z_logvar, interpolation_range, raw_model):\n",
    "\n",
    "    init_test = True\n",
    "    for interpolation in interpolation_range:\n",
    "\n",
    "        inter_z_mu = torch.add( torch.mul(test1_z_mu, (1-interpolation)), torch.mul(test2_z_mu, interpolation) )\n",
    "        inter_z_logvar = torch.add( torch.mul(test1_z_logvar, (1-interpolation)), torch.mul(test2_z_logvar, interpolation) )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_pred_z = raw_model.reparameterize(inter_z_mu, inter_z_logvar)\n",
    "            test_pred = raw_model.decode(test_pred_z)\n",
    "\n",
    "        if init_test:\n",
    "            test_predictions = test_pred\n",
    "            init_test = False\n",
    "\n",
    "        else:\n",
    "            test_predictions = torch.cat((test_predictions, test_pred ),0)\n",
    "        \n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cl7AjpGTtpn5",
    "outputId": "497d55c0-5d55-4a74-e862-b43b0a119bc2"
   },
   "outputs": [],
   "source": [
    "interpolation_range = np.arange(0, 1.1, 0.2)\n",
    "\n",
    "inter_raw_all = raw_interpolate_stepwise_z_dist(test1_z_mu, test1_z_logvar, test2_z_mu, test2_z_logvar, interpolation_range, raw_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbH4ZKsxSOhS"
   },
   "outputs": [],
   "source": [
    "inter_raw_all_np = inter_raw_all.view(-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "id": "e4OOtZOoRYZK",
    "outputId": "d9d3f80b-ad3d-4cfb-ac3e-07319471d014"
   },
   "outputs": [],
   "source": [
    "display.Audio(inter_raw_all_np, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuaPEY4UwDVr"
   },
   "outputs": [],
   "source": [
    "out_path  = './content/my_audio.wav'\n",
    "sf.write(out_path, inter_raw_all_np, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "ZVWDsXTYRf4h",
    "outputId": "5fb28350-6be5-40cf-d05f-384b7f0713e5"
   },
   "outputs": [],
   "source": [
    "librosa.display.waveshow(inter_raw_all_np, sr=sr, color=\"magenta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "JyCbRsRTSKx6",
    "outputId": "b65755a4-1111-407c-9dfb-c5c44bae6f2c"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(inter_raw_all_np, hop_length=hop_length)),ref=np.max)\n",
    "img = librosa.display.specshow(D, y_axis='log', sr=sr, hop_length=hop_length, x_axis='time', ax=ax)\n",
    "ax.set(title='Log-frequency power spectrogram')\n",
    "ax.label_outer()\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WIxgNZ6SUkpa"
   },
   "source": [
    "\n",
    "## Interpolations in Meso-scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gdrg7pSxyz1"
   },
   "outputs": [],
   "source": [
    "def concat_random_audio(audio_files, duration, sampling_rate):\n",
    "    \n",
    "    # concat random files from the dataset until the array is [duration] secs long.\n",
    "\n",
    "    array_length = 0\n",
    "\n",
    "    init = True\n",
    "\n",
    "    while array_length < (duration * sampling_rate):\n",
    "        \n",
    "        path = audio_files[random.randint(0, len(audio_files)- 1)]\n",
    "        \n",
    "        y, fs = librosa.load(path, sr=None)\n",
    "        \n",
    "        if init:\n",
    "            audio_1 = y\n",
    "        else:\n",
    "            audio_1 = np.concatenate((audio_1, y), 0)\n",
    "\n",
    "        init = False\n",
    "        array_length = audio_1.shape[0]\n",
    "\n",
    "    init = True\n",
    "    array_length = 0\n",
    "\n",
    "    while array_length < (duration * sampling_rate):\n",
    "        \n",
    "        path = audio_files[random.randint(0, len(audio_files)- 1)]\n",
    "        \n",
    "        y, fs = librosa.load(path, sr=None)\n",
    "        \n",
    "        if init:\n",
    "            audio_2 = y\n",
    "        else:\n",
    "            audio_2 = np.concatenate((audio_2, y), 0)\n",
    "\n",
    "        init = False\n",
    "        array_length = audio_2.shape[0]\n",
    "\n",
    "    return audio_1[:duration * sampling_rate], audio_2[:duration * sampling_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y03M2WMR2DWp"
   },
   "outputs": [],
   "source": [
    "audio_1, audio_2 = concat_random_audio(lts_audio_files, duration=120, sampling_rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "5Cidiaa02E1P",
    "outputId": "31e7e2d7-e3dd-4c01-aeec-ab7c5f955844"
   },
   "outputs": [],
   "source": [
    "librosa.display.waveshow(audio_1, sr=sr,color=\"magenta\")\n",
    "display.Audio(audio_1, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "Qs9H5Qu72Gd6",
    "outputId": "2033f20a-7071-4a4a-de87-7f5eff672a77"
   },
   "outputs": [],
   "source": [
    "librosa.display.waveshow(audio_2, sr=sr, color=\"magenta\")\n",
    "display.Audio(audio_2, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x1vinlc2Vfm"
   },
   "outputs": [],
   "source": [
    "som_clusters_path = r\"./content/2022-zkm-workshop/ltsp/erokia/som/clusters.json\"\n",
    "som_data_path = r\"./content/2022-zkm-workshop/ltsp/erokia/som/data-concatenated.json\"\n",
    "\n",
    "with open(som_clusters_path, 'r') as f:\n",
    "  som_clusters_dict = json.load(f)\n",
    "\n",
    "with open(som_data_path, 'r') as f:\n",
    "  som_data_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9niyWqe2awH"
   },
   "outputs": [],
   "source": [
    "def concat_audio_som(audio_files, sampling_rate, cluster_idx, som_clusters_dict, som_data_dict):\n",
    "    init = True\n",
    "    cluster = som_clusters_dict[str(cluster_idx)]\n",
    "\n",
    "    for index in cluster:\n",
    "    \n",
    "        path = som_data_dict[str(index)][1]\n",
    "        path = audio_files.joinpath(path)\n",
    "        y, fs = librosa.load(path, sr=None)\n",
    "        \n",
    "        if init:\n",
    "            audio = y\n",
    "        else:\n",
    "            audio = np.concatenate((audio, y), 0)\n",
    "\n",
    "        init = False\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KzpLPHRUdNP",
    "outputId": "7a261145-d3d2-42d0-e2cb-5566e9441381"
   },
   "outputs": [],
   "source": [
    "cluster_idx = 18\n",
    "test_audio_1 = concat_audio_som(audio, sampling_rate, cluster_idx, som_clusters_dict, som_data_dict)\n",
    "len(test_audio_1) / sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKHlXwoLXS38"
   },
   "outputs": [],
   "source": [
    "librosa.display.waveshow(test_audio_1, sr=sr, color=\"magenta\")\n",
    "display.Audio(test_audio_1, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQL68pykXPMB",
    "outputId": "fc39a136-8218-4819-f1d7-d69f697aa3a0"
   },
   "outputs": [],
   "source": [
    "cluster_idx = 24\n",
    "test_audio_2 = concat_audio_som(audio, sampling_rate, cluster_idx, som_clusters_dict, som_data_dict)\n",
    "len(test_audio_2) / sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "s9wpQw1fXPoC",
    "outputId": "e9aaef20-e863-49ea-ddb6-a052bf97a44c"
   },
   "outputs": [],
   "source": [
    "librosa.display.waveshow(test_audio_2, sr=sr, color=\"magenta\")\n",
    "display.Audio(test_audio_2, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkYW-PP427_5"
   },
   "outputs": [],
   "source": [
    "def match_audio_size(audio_1, audio_2, match_size = 1):\n",
    "    # We should match the audio lengths\n",
    "    # 0 is crop the longer, \n",
    "    # 1 is repeat the shorter, \n",
    "    #  \n",
    "\n",
    "    match_size = 1\n",
    "\n",
    "    if match_size == 0:\n",
    "        if audio_1.shape[0] < audio_2.shape[0]:\n",
    "            audio_2 = audio_2[:audio_1.shape[0]]\n",
    "        else:\n",
    "            audio_1 = audio_1[:audio_2.shape[0]]\n",
    "\n",
    "    if match_size == 1:\n",
    "        if audio_1.shape[0] < audio_2.shape[0]:\n",
    "            while audio_1.shape[0] < audio_2.shape[0]:\n",
    "                audio_1 = np.concatenate((audio_1, audio_1), 0)\n",
    "            audio_1 = audio_1[:audio_2.shape[0]] \n",
    "\n",
    "        else:\n",
    "            while audio_2.shape[0] < audio_1.shape[0]:\n",
    "                audio_2 = np.concatenate((audio_2, audio_2), 0)\n",
    "            audio_2 = audio_2[:audio_1.shape[0]]\n",
    "    return audio_1, audio_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo3y4CbEXWVP"
   },
   "outputs": [],
   "source": [
    "test_audio_1, test_audio_2 = match_audio_size(test_audio_1, test_audio_2, match_size = 1)\n",
    "\n",
    "long_dataset1 = TestDataset(test_audio_1, segment_length = segment_length, sampling_rate = sampling_rate, transform=ToTensor())\n",
    "long_dataloader1 = DataLoader(long_dataset1, batch_size = batch_size, shuffle=False)\n",
    "long1_z_mu, long1_z_logvar = raw_to_z_dist(long_dataloader1, raw_model, device)\n",
    "\n",
    "long_dataset2 = TestDataset(test_audio_2, segment_length = segment_length, sampling_rate = sampling_rate, transform=ToTensor())\n",
    "long_dataloader2 = DataLoader(long_dataset2, batch_size = batch_size, shuffle=False)\n",
    "long2_z_mu, long2_z_logvar = raw_to_z_dist(long_dataloader2, raw_model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ksv2NsnXYM_"
   },
   "outputs": [],
   "source": [
    "# interpolation = np.sin(np.linspace(-10* np.pi, 10 * np.pi, 20000))\n",
    "interpolation = np.sin(np.linspace(-500* np.pi, 500* np.pi, 20000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvZ6tskOXZJq",
    "outputId": "70a36781-5593-43a7-f835-d887c9a230a0"
   },
   "outputs": [],
   "source": [
    "  \n",
    "# match the size of interpolation array to audio array using scipy interpolation \n",
    "f_stretch = sp_interpolate.interp1d(np.arange(0, len(interpolation)), interpolation)\n",
    "my_stretched_alfa = f_stretch(np.linspace(0.0, len(interpolation)-1, len(long1_z_mu)))\n",
    "my_stretched_alfa = torch.from_numpy(my_stretched_alfa).to(device)\n",
    "\n",
    "# repeat the alfa values to 256 latent dimensions, that is, each alfa value is a constant multiplier for a latent vector\n",
    "my_stretched_alfa = torch.repeat_interleave(my_stretched_alfa.unsqueeze(1), long1_z_mu.shape[1], axis=1)\n",
    "\n",
    "#generate mixed latent vectors    \n",
    "#alfa(latent1-latent2)+latent2 = alfa * latent1 + (1-alfa) * latent2\n",
    "inter_z_mu = torch.add( torch.mul(long1_z_mu, (1-my_stretched_alfa)), torch.mul(long2_z_mu, my_stretched_alfa) )\n",
    "inter_z_logvar = torch.add( torch.mul(long1_z_logvar, (1-my_stretched_alfa)), torch.mul(long2_z_logvar, my_stretched_alfa) )\n",
    "\n",
    "init_test = True\n",
    "with torch.no_grad():\n",
    "     \n",
    "    test_pred_z = raw_model.reparameterize(inter_z_mu, inter_z_logvar)\n",
    "    test_pred = raw_model.decode(test_pred_z.float())\n",
    "\n",
    "    if init_test:\n",
    "        test_predictions = test_pred\n",
    "        init_test = False\n",
    "\n",
    "    else:\n",
    "        test_predictions = torch.cat((test_predictions, test_pred ),0)\n",
    "\n",
    "out_audio = test_predictions.view(-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMhNTa1mXiKY"
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "out_path  = './content/normal.wav'\n",
    "sf.write(out_path, out_audio, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "15xKk6bt_uXG",
    "outputId": "c0f60595-dc19-4e90-8b45-3f7b60b34cc8"
   },
   "outputs": [],
   "source": [
    "librosa.display.waveshow(out_audio, sr=sr,color=\"magenta\")\n",
    "display.Audio(out_audio, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNnr7YA-SfS4"
   },
   "source": [
    "## Interpolations with Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pa6qnEbvSiyd"
   },
   "outputs": [],
   "source": [
    "def concat_random_audio(audio_files, duration, sampling_rate):\n",
    "    \n",
    "    # concat random files from the dataset until the array is [duration] secs long.\n",
    "\n",
    "    array_length = 0\n",
    "\n",
    "    init = True\n",
    "\n",
    "    while array_length < (duration * sampling_rate):\n",
    "        \n",
    "        path = audio_files[random.randint(0, len(audio_files)- 1)]\n",
    "        \n",
    "        y, fs = librosa.load(path, sr=None)\n",
    "        \n",
    "        if init:\n",
    "            audio_1 = y\n",
    "        else:\n",
    "            audio_1 = np.concatenate((audio_1, y), 0)\n",
    "\n",
    "        init = False\n",
    "        array_length = audio_1.shape[0]\n",
    "\n",
    "    init = True\n",
    "    array_length = 0\n",
    "\n",
    "    while array_length < (duration * sampling_rate):\n",
    "        \n",
    "        path = audio_files[random.randint(0, len(audio_files)- 1)]\n",
    "        \n",
    "        y, fs = librosa.load(path, sr=None)\n",
    "        \n",
    "        if init:\n",
    "            audio_2 = y\n",
    "        else:\n",
    "            audio_2 = np.concatenate((audio_2, y), 0)\n",
    "\n",
    "        init = False\n",
    "        array_length = audio_2.shape[0]\n",
    "\n",
    "    return audio_1[:duration * sampling_rate], audio_2[:duration * sampling_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjW40ptnSWMs"
   },
   "outputs": [],
   "source": [
    "audio_1, audio_2 = concat_random_audio(lts_audio_files, duration=120, sampling_rate=sampling_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "HzFOkg9nSxsv",
    "outputId": "51e46b0f-ecb7-465a-cae1-853d41f71b0b"
   },
   "outputs": [],
   "source": [
    "librosa.display.waveshow(audio_1, sr=sr,color=\"magenta\")\n",
    "display.Audio(audio_1, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "iDUI1Rq0SxUg",
    "outputId": "bd6704cc-216c-47cb-eff8-5ebf39570a32"
   },
   "outputs": [],
   "source": [
    "librosa.display.waveshow(audio_2, sr=sr, color=\"magenta\")\n",
    "display.Audio(audio_2, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJb3A1ROSsVU"
   },
   "outputs": [],
   "source": [
    "def concat_audio_som(audio_files, sampling_rate, cluster_idx, som_clusters_dict, som_data_dict):\n",
    "    init = True\n",
    "    cluster = som_clusters_dict[str(cluster_idx)]\n",
    "\n",
    "    for index in cluster:\n",
    "    \n",
    "        path = som_data_dict[str(index)][1]\n",
    "        path = audio_files.joinpath(path)\n",
    "        y, fs = librosa.load(path, sr=None)\n",
    "        \n",
    "        if init:\n",
    "            audio = y\n",
    "        else:\n",
    "            audio = np.concatenate((audio, y), 0)\n",
    "\n",
    "        init = False\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sq7H5Mg6Svhf"
   },
   "outputs": [],
   "source": [
    "som_clusters_path = r\"./content/2022-zkm-workshop/ltsp/erokia/som/clusters.json\"\n",
    "som_data_path = r\"./content/2022-zkm-workshop/ltsp/erokia/som/data-concatenated.json\"\n",
    "\n",
    "with open(som_clusters_path, 'r') as f:\n",
    "  som_clusters_dict = json.load(f)\n",
    "\n",
    "with open(som_data_path, 'r') as f:\n",
    "  som_data_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3knyNwnTYEf",
    "outputId": "6323c526-b12e-43c8-9ce4-96abce6b0822"
   },
   "outputs": [],
   "source": [
    "cluster_idx = 32\n",
    "test_audio_1 = concat_audio_som(audio, sampling_rate, cluster_idx, som_clusters_dict, som_data_dict)\n",
    "len(test_audio_1) / sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltPMbeWoTdse",
    "outputId": "f0a2573d-871d-44a5-fa6e-cbd4a11252b8"
   },
   "outputs": [],
   "source": [
    "cluster_idx = 46\n",
    "test_audio_2 = concat_audio_som(audio, sampling_rate, cluster_idx, som_clusters_dict, som_data_dict)\n",
    "len(test_audio_2) / sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "zJgPJ-4iTe7j",
    "outputId": "965dd989-23e8-4e58-9f58-7f7d551c67af"
   },
   "outputs": [],
   "source": [
    "librosa.display.waveshow(test_audio_1, sr=sr,color=\"magenta\")\n",
    "display.Audio(test_audio_1, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "z9R5xjpPTkNw",
    "outputId": "39f464f8-588b-4a96-eaba-3bcb35bcf43d"
   },
   "outputs": [],
   "source": [
    "librosa.display.waveshow(test_audio_2, sr=sr,color=\"magenta\")\n",
    "display.Audio(test_audio_2, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iL9KPIyvUHYf"
   },
   "outputs": [],
   "source": [
    "def match_audio_size(audio_1, audio_2, match_size = 1):\n",
    "    # We should match the audio lengths\n",
    "    # 0 is crop the longer, \n",
    "    # 1 is repeat the shorter, \n",
    "    #  \n",
    "\n",
    "    match_size = 1\n",
    "\n",
    "    if match_size == 0:\n",
    "        if audio_1.shape[0] < audio_2.shape[0]:\n",
    "            audio_2 = audio_2[:audio_1.shape[0]]\n",
    "        else:\n",
    "            audio_1 = audio_1[:audio_2.shape[0]]\n",
    "\n",
    "    if match_size == 1:\n",
    "        if audio_1.shape[0] < audio_2.shape[0]:\n",
    "            while audio_1.shape[0] < audio_2.shape[0]:\n",
    "                audio_1 = np.concatenate((audio_1, audio_1), 0)\n",
    "            audio_1 = audio_1[:audio_2.shape[0]] \n",
    "\n",
    "        else:\n",
    "            while audio_2.shape[0] < audio_1.shape[0]:\n",
    "                audio_2 = np.concatenate((audio_2, audio_2), 0)\n",
    "            audio_2 = audio_2[:audio_1.shape[0]]\n",
    "    return audio_1, audio_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvlQsLAjTmiD"
   },
   "outputs": [],
   "source": [
    "test_audio_1, test_audio_2 = match_audio_size(test_audio_1, test_audio_2, match_size = 1)\n",
    "\n",
    "long_dataset1 = AudioDataset(test_audio_1, segment_length = segment_length, sampling_rate = sampling_rate, hop_size = hop_length, transform=ToTensor())\n",
    "long_dataloader1 = DataLoader(long_dataset1, batch_size = batch_size, shuffle=False)\n",
    "long1_z_mu, long1_z_logvar = raw_to_z_dist(long_dataloader1, raw_model, device)\n",
    "\n",
    "long_dataset2 = AudioDataset(test_audio_2, segment_length = segment_length, sampling_rate = sampling_rate, hop_size = hop_length, transform=ToTensor())\n",
    "long_dataloader2 = DataLoader(long_dataset2, batch_size = batch_size, shuffle=False)\n",
    "long2_z_mu, long2_z_logvar = raw_to_z_dist(long_dataloader2, raw_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjb1TIkkUJ7D"
   },
   "outputs": [],
   "source": [
    "interpolation = np.sin(np.linspace(-np.pi, np.pi, 2000))\n",
    "# interpolation = np.clip(interpolation * 10, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emDMbwWMUO9l",
    "outputId": "6be42896-aa7c-4f88-dcab-c8008f555107"
   },
   "outputs": [],
   "source": [
    "  # match the size of interpolation array to audio array using scipy interpolation \n",
    "f_stretch = sp_interpolate.interp1d(np.arange(0, len(interpolation)), interpolation)\n",
    "my_stretched_alfa = f_stretch(np.linspace(0.0, len(interpolation)-1, len(long1_z_mu)))\n",
    "my_stretched_alfa = torch.from_numpy(my_stretched_alfa).to(device)\n",
    "\n",
    "# repeat the alfa values to 256 latent dimensions, that is, each alfa value is a constant multiplier for a latent vector\n",
    "my_stretched_alfa = torch.repeat_interleave(my_stretched_alfa.unsqueeze(1), long1_z_mu.shape[1], axis=1)\n",
    "\n",
    "#generate mixed latent vectors    \n",
    "#alfa(latent1-latent2)+latent2 = alfa * latent1 + (1-alfa) * latent2\n",
    "inter_z_mu = torch.add( torch.mul(long1_z_mu, (1-my_stretched_alfa)), torch.mul(long2_z_mu, my_stretched_alfa) )\n",
    "inter_z_logvar = torch.add( torch.mul(long1_z_logvar, (1-my_stretched_alfa)), torch.mul(long2_z_logvar, my_stretched_alfa) )\n",
    "\n",
    "init_test = True\n",
    "with torch.no_grad():\n",
    "     \n",
    "    test_pred_z = raw_model.reparameterize(inter_z_mu, inter_z_logvar)\n",
    "    test_pred = raw_model.decode(test_pred_z.float())\n",
    "\n",
    "    if init_test:\n",
    "        test_predictions = test_pred\n",
    "        init_test = False\n",
    "\n",
    "    else:\n",
    "        test_predictions = torch.cat((test_predictions, test_pred ),0)\n",
    "\n",
    "\n",
    "out_audio = test_predictions.view(-1).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcdU7_OUWKLN"
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "out_path  = './content/hacky.wav'\n",
    "sf.write(out_path, out_audio, sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "This code is built within the following research residency:\n",
    "\n",
    "https://kivanctatar.com/Coding-Latent-No-1\n",
    "\n",
    "This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program – Humanities and Society (WASP-HS) funded by the Marianne and Marcus Wallenberg Foundation and the Marcus and Amalia Wallenberg Foundation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GLhSNjjgERrr",
    "VaOjyqxHg-FP",
    "vzmoFaAunkIx",
    "eH70BnNZoD2V"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
